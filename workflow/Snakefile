import pandas as pd
from pathlib import Path
import os
from scripts.samples_auto import read_sample


# load the configuration file
configfile: 'config/config.yaml'


# load sample sheet from a TSV file or create new from samples in 'data' folder
if config['data']['sample_sheet_path']:
  samples = pd.read_csv(config['data']['sample_sheet_path'], index_col = 'sample_name', sep = '\t')
elif config['samples_in_directory']:
  samples = read_sample(config['data']['samples_in_directory'])
else:
  print('no samples detected')
wc = samples.index # store samples in wildcard


# location of conda environments
if config['conda_envs']['use_existing_envs']:
  env_prefix = config['conda_envs']['envs_path']
  env_suffix = ''
else:
  env_prefix = '../envs/'
  env_suffix = '.yaml'


# set variables depending on whether to use subset or full dataset
if config['subset']['use']:
    filename = 'subset'
else:
    filename = 'merged'


include: 'rules/preprocessing.smk'
include: 'rules/analyse_merged_ds.smk'

rule all:
    input:
        'results/preprocessing/subset.h5ad',

        expand('results/analysis/' + filename + '/dim_reduc/checkpoints/{layer_to_use}__{scale_data_before_pca}__{genes_for_pca}__{cc_method}__{pca_n_components}__{umap_n_neighbors}.done',
            layer_to_use = config['dim_reduc']['layer_to_use'],
            scale_data_before_pca = config['dim_reduc']['scale_data_before_pca'],
            genes_for_pca = config['dim_reduc']['genes_for_pca'],
            cc_method = config['dim_reduc']['cc_method'],
            pca_n_components = config['dim_reduc']['pca_n_components'],
            umap_n_neighbors = config['dim_reduc']['umap_n_neighbors']
        ),
        # expand('results/preprocessing/{sample}/annotation.h5ad', sample=wc),
        # expand('results/preprocessing/{sample}/scAutoQC.h5ad', sample=wc),